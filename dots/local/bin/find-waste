#!/usr/bin/env bash
#
# disk-waste-finder.sh
# Finds common sources of wasted disk space in and around source code.
#
# Usage: ./disk-waste-finder.sh [target_directory] [--delete] [--json]
#
# --delete   Interactively prompt to remove found waste
# --json     Output results as JSON (skips interactive mode)
#

set -u

# ---------------------------------------------------------------------------
# Config / thresholds
# ---------------------------------------------------------------------------
NODE_MODULES_THRESHOLD_MB=100
VENV_THRESHOLD_MB=200
BUILD_DIR_THRESHOLD_MB=50
LOG_FILE_THRESHOLD_MB=10
CACHE_DIR_THRESHOLD_MB=50
DUPLICATE_LOCKFILE_WARN=true
STALE_BRANCH_DAYS=90
LARGE_BINARY_THRESHOLD_MB=5
LARGE_MEDIA_THRESHOLD_MB=10

# ---------------------------------------------------------------------------
# Color and formatting
# ---------------------------------------------------------------------------
RED='\033[0;31m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
GREEN='\033[0;32m'
BOLD='\033[1m'
DIM='\033[2m'
RESET='\033[0m'

# ---------------------------------------------------------------------------
# Parse args
# ---------------------------------------------------------------------------
TARGET_DIR="${1:-.}"
DELETE_MODE=false
JSON_MODE=false
TOTAL_WASTE=0
FINDINGS=()

for arg in "$@"; do
    case "$arg" in
        --delete) DELETE_MODE=true ;;
        --json)   JSON_MODE=true ;;
    esac
done

if [[ ! -d "$TARGET_DIR" ]]; then
    echo "Error: '$TARGET_DIR' is not a directory." >&2
    exit 1
fi

TARGET_DIR="$(cd "$TARGET_DIR" && pwd)"

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------
human_size() {
    local bytes=$1
    if (( bytes >= 1073741824 )); then
        printf "%.1f GB" "$(echo "scale=1; $bytes / 1073741824" | bc)"
    elif (( bytes >= 1048576 )); then
        printf "%.1f MB" "$(echo "scale=1; $bytes / 1048576" | bc)"
    elif (( bytes >= 1024 )); then
        printf "%.1f KB" "$(echo "scale=1; $bytes / 1024" | bc)"
    else
        printf "%d B" "$bytes"
    fi
}

dir_size_bytes() {
    local result
    result=$(du -sb "$1" 2>/dev/null | awk '{print $1}')
    echo "${result:-0}"
}

dir_size_mb() {
    local bytes
    bytes=$(dir_size_bytes "$1")
    echo $(( ${bytes:-0} / 1048576 ))
}

file_size_bytes() {
    local result
    result=$(stat -c%s "$1" 2>/dev/null || echo 0)
    echo "${result:-0}"
}

record_finding() {
    local category="$1"
    local path="$2"
    local size_bytes="$3"
    local description="$4"

    TOTAL_WASTE=$(( TOTAL_WASTE + size_bytes ))
    local hsize
    hsize=$(human_size "$size_bytes")

    if $JSON_MODE; then
        FINDINGS+=("{\"category\":\"$category\",\"path\":\"$path\",\"bytes\":$size_bytes,\"human_size\":\"$hsize\",\"description\":\"$description\"}")
    else
        printf "  ${RED}%-10s${RESET} ${BOLD}%-50s${RESET} ${YELLOW}%10s${RESET}  %s\n" \
            "$category" "${path#"$TARGET_DIR"/}" "$hsize" "$description"
    fi
}

maybe_delete() {
    local path="$1"
    local description="$2"
    if $DELETE_MODE && ! $JSON_MODE; then
        printf "  ${CYAN}Delete ${path#"$TARGET_DIR"/}? [y/N]:${RESET} "
        read -r confirm
        if [[ "$confirm" =~ ^[Yy]$ ]]; then
            rm -rf "$path"
            printf "  ${GREEN}Deleted.${RESET}\n"
        fi
    fi
}

section_header() {
    if ! $JSON_MODE; then
        echo ""
        printf "${BOLD}${CYAN}[%s]${RESET}\n" "$1"
        printf "${DIM}%s${RESET}\n" "$2"
    fi
}

# ---------------------------------------------------------------------------
# 1. Bloated dependency directories
# ---------------------------------------------------------------------------
section_header "DEPENDENCY BLOAT" "node_modules, vendor dirs, virtualenvs beyond reasonable size"

# node_modules
while IFS= read -r -d '' dir; do
    size_mb=$(dir_size_mb "$dir")
    if (( size_mb >= NODE_MODULES_THRESHOLD_MB )); then
        size_bytes=$(dir_size_bytes "$dir")
        record_finding "DEPS" "$dir" "$size_bytes" "node_modules >= ${NODE_MODULES_THRESHOLD_MB}MB"
        maybe_delete "$dir" "node_modules"
    fi
done < <(find "$TARGET_DIR" -name "node_modules" -type d -prune -print0 2>/dev/null)

# Python virtualenvs (venv, .venv, env, .env with pyvenv.cfg)
while IFS= read -r -d '' cfg; do
    venv_dir="$(dirname "$cfg")"
    size_mb=$(dir_size_mb "$venv_dir")
    if (( size_mb >= VENV_THRESHOLD_MB )); then
        size_bytes=$(dir_size_bytes "$venv_dir")
        record_finding "DEPS" "$venv_dir" "$size_bytes" "Python venv >= ${VENV_THRESHOLD_MB}MB"
        maybe_delete "$venv_dir" "virtualenv"
    fi
done < <(find "$TARGET_DIR" -name "pyvenv.cfg" -print0 2>/dev/null)

# Ruby vendor/bundle
while IFS= read -r -d '' dir; do
    if [[ -d "$dir/ruby" ]] || [[ -d "$dir/bundle" ]]; then
        size_mb=$(dir_size_mb "$dir")
        if (( size_mb >= NODE_MODULES_THRESHOLD_MB )); then
            size_bytes=$(dir_size_bytes "$dir")
            record_finding "DEPS" "$dir" "$size_bytes" "Ruby vendor bundle"
            maybe_delete "$dir" "vendor/bundle"
        fi
    fi
done < <(find "$TARGET_DIR" -path "*/vendor" -type d -prune -print0 2>/dev/null)

# Go module cache inside projects
while IFS= read -r -d '' dir; do
    size_mb=$(dir_size_mb "$dir")
    if (( size_mb >= BUILD_DIR_THRESHOLD_MB )); then
        size_bytes=$(dir_size_bytes "$dir")
        record_finding "DEPS" "$dir" "$size_bytes" "Go module cache in project"
    fi
done < <(find "$TARGET_DIR" -path "*/go/pkg/mod" -type d -prune -print0 2>/dev/null)

# ---------------------------------------------------------------------------
# 2. Build artifacts and output dirs
# ---------------------------------------------------------------------------
section_header "BUILD ARTIFACTS" "Compiled output, dist dirs, build caches left behind"

BUILD_PATTERNS=(
    "dist"
    "build"
    "out"
    "target"         # Rust/Java/Maven
    ".next"          # Next.js
    ".nuxt"          # Nuxt
    ".output"        # Nitro/Nuxt
    "__pycache__"
    ".pytest_cache"
    ".mypy_cache"
    ".tox"
    "*.egg-info"
    ".gradle"
    ".cargo/registry"
)

for pattern in "${BUILD_PATTERNS[@]}"; do
    while IFS= read -r -d '' dir; do
        # Skip if inside node_modules or .git
        [[ "$dir" == *"/node_modules/"* ]] && continue
        [[ "$dir" == *"/.git/"* ]] && continue

        size_mb=$(dir_size_mb "$dir")
        if (( size_mb >= BUILD_DIR_THRESHOLD_MB )); then
            size_bytes=$(dir_size_bytes "$dir")
            record_finding "BUILD" "$dir" "$size_bytes" "Build artifact >= ${BUILD_DIR_THRESHOLD_MB}MB"
            maybe_delete "$dir" "build artifact"
        fi
    done < <(find "$TARGET_DIR" -name "$pattern" -type d -prune -print0 2>/dev/null)
done

# ---------------------------------------------------------------------------
# 3. Cache directories
# ---------------------------------------------------------------------------
section_header "CACHES" "Package manager and tool caches"

CACHE_PATTERNS=(
    ".cache"
    ".npm"
    ".yarn/cache"
    ".pnpm-store"
    ".eslintcache"
    ".parcel-cache"
    ".turbo"
    ".webpack"
    "__pycache__"
    ".sass-cache"
)

for pattern in "${CACHE_PATTERNS[@]}"; do
    while IFS= read -r -d '' dir; do
        [[ "$dir" == *"/node_modules/"* ]] && continue
        [[ "$dir" == *"/.git/"* ]] && continue

        size_mb=$(dir_size_mb "$dir")
        if (( size_mb >= CACHE_DIR_THRESHOLD_MB )); then
            size_bytes=$(dir_size_bytes "$dir")
            record_finding "CACHE" "$dir" "$size_bytes" "Cache dir >= ${CACHE_DIR_THRESHOLD_MB}MB"
            maybe_delete "$dir" "cache"
        fi
    done < <(find "$TARGET_DIR" -name "$pattern" -type d -prune -print0 2>/dev/null)
done

# ---------------------------------------------------------------------------
# 4. Large log files
# ---------------------------------------------------------------------------
section_header "LOG FILES" "Logs that have grown beyond ${LOG_FILE_THRESHOLD_MB}MB"

while IFS= read -r -d '' f; do
    [[ "$f" == *"/node_modules/"* ]] && continue
    [[ "$f" == *"/.git/"* ]] && continue

    size_bytes=$(file_size_bytes "$f")
    size_mb=$(( size_bytes / 1048576 ))
    if (( size_mb >= LOG_FILE_THRESHOLD_MB )); then
        record_finding "LOG" "$f" "$size_bytes" "Log file >= ${LOG_FILE_THRESHOLD_MB}MB"
        maybe_delete "$f" "log file"
    fi
done < <(find "$TARGET_DIR" \( -name "*.log" -o -name "*.log.*" -o -name "npm-debug.log*" \
    -o -name "yarn-debug.log*" -o -name "yarn-error.log*" -o -name "lerna-debug.log*" \) \
    -type f -print0 2>/dev/null)

# ---------------------------------------------------------------------------
# 5. Large binaries committed to source trees
# ---------------------------------------------------------------------------
section_header "LARGE BINARIES IN SRC" "Binary blobs that probably should not live in a source tree"

BINARY_EXTENSIONS=(
    "*.exe" "*.dll" "*.so" "*.dylib" "*.a" "*.lib"
    "*.jar" "*.war" "*.ear"
    "*.o" "*.obj" "*.pyc" "*.pyo"
    "*.whl" "*.gem"
    "*.bin" "*.dat"
    "*.db" "*.sqlite" "*.sqlite3"
    "*.iso" "*.img" "*.dmg"
    "*.tar" "*.tar.gz" "*.tgz" "*.tar.bz2" "*.tar.xz"
    "*.zip" "*.rar" "*.7z" "*.gz" "*.bz2" "*.xz" "*.zst"
    "*.deb" "*.rpm" "*.msi"
    "*.pkl" "*.pickle" "*.pt" "*.pth" "*.onnx" "*.h5" "*.hdf5"  # ML models
    "*.safetensors" "*.gguf" "*.ggml"                              # LLM weights
)

find_expr=()
for ext in "${BINARY_EXTENSIONS[@]}"; do
    if [[ ${#find_expr[@]} -gt 0 ]]; then
        find_expr+=("-o")
    fi
    find_expr+=("-name" "$ext")
done

while IFS= read -r -d '' f; do
    [[ "$f" == *"/node_modules/"* ]] && continue
    [[ "$f" == *"/.git/"* ]] && continue
    [[ "$f" == *"/.venv/"* ]] && continue
    [[ "$f" == *"/venv/"* ]] && continue

    size_bytes=$(file_size_bytes "$f")
    size_mb=$(( size_bytes / 1048576 ))
    if (( size_mb >= LARGE_BINARY_THRESHOLD_MB )); then
        record_finding "BINARY" "$f" "$size_bytes" "Large binary in source tree"
        maybe_delete "$f" "binary blob"
    fi
done < <(find "$TARGET_DIR" -type f \( "${find_expr[@]}" \) -print0 2>/dev/null)

# ---------------------------------------------------------------------------
# 6. Large media files in source trees
# ---------------------------------------------------------------------------
section_header "MEDIA IN SRC" "Large images, videos, audio that might belong in object storage or LFS"

MEDIA_EXTENSIONS=(
    "*.mp4" "*.avi" "*.mkv" "*.mov" "*.wmv" "*.flv" "*.webm"
    "*.mp3" "*.wav" "*.flac" "*.aac" "*.ogg" "*.m4a" "*.wma"
    "*.psd" "*.ai" "*.sketch" "*.fig"
    "*.tif" "*.tiff" "*.bmp" "*.raw" "*.cr2" "*.nef" "*.dng"
    "*.pdf"
)

find_media_expr=()
for ext in "${MEDIA_EXTENSIONS[@]}"; do
    if [[ ${#find_media_expr[@]} -gt 0 ]]; then
        find_media_expr+=("-o")
    fi
    find_media_expr+=("-iname" "$ext")
done

while IFS= read -r -d '' f; do
    [[ "$f" == *"/node_modules/"* ]] && continue
    [[ "$f" == *"/.git/"* ]] && continue

    size_bytes=$(file_size_bytes "$f")
    size_mb=$(( size_bytes / 1048576 ))
    if (( size_mb >= LARGE_MEDIA_THRESHOLD_MB )); then
        record_finding "MEDIA" "$f" "$size_bytes" "Large media file in source tree"
        maybe_delete "$f" "media file"
    fi
done < <(find "$TARGET_DIR" -type f \( "${find_media_expr[@]}" \) -print0 2>/dev/null)

# ---------------------------------------------------------------------------
# 7. Bloated .git directories
# ---------------------------------------------------------------------------
section_header "GIT BLOAT" "Oversized .git directories (may need gc or history rewrite)"

while IFS= read -r -d '' dir; do
    size_bytes=$(dir_size_bytes "$dir")
    [[ -z "$size_bytes" || "$size_bytes" -eq 0 ]] && continue
    size_mb=$(( size_bytes / 1048576 ))
    # Compare .git size to working tree size
    repo_dir="$(dirname "$dir")"
    working_bytes=$(dir_size_bytes "$repo_dir")
    working_bytes=${working_bytes:-0}
    git_ratio=0
    if (( working_bytes > 0 )); then
        git_ratio=$(( (size_bytes * 100) / working_bytes ))
    fi

    # Flag if .git is over 500MB or is more than 60% of total repo size
    if (( size_mb >= 500 )) || (( git_ratio >= 60 && size_mb >= 100 )); then
        record_finding "GIT" "$dir" "$size_bytes" ".git is ${git_ratio}% of repo (try: git gc --aggressive)"
    fi
done < <(find "$TARGET_DIR" -name ".git" -type d -prune -print0 2>/dev/null)

# ---------------------------------------------------------------------------
# 8. Docker waste
# ---------------------------------------------------------------------------
section_header "DOCKER" "Docker-related waste in project directories"

# Check for .docker directories with large build caches
while IFS= read -r -d '' dir; do
    size_mb=$(dir_size_mb "$dir")
    if (( size_mb >= BUILD_DIR_THRESHOLD_MB )); then
        size_bytes=$(dir_size_bytes "$dir")
        record_finding "DOCKER" "$dir" "$size_bytes" "Docker build context/cache"
        maybe_delete "$dir" "docker cache"
    fi
done < <(find "$TARGET_DIR" -name ".docker" -type d -prune -print0 2>/dev/null)

# ---------------------------------------------------------------------------
# 9. Duplicate / redundant lockfiles (warning only)
# ---------------------------------------------------------------------------
section_header "LOCKFILE CONFLICTS" "Multiple package managers detected in same project (not a size issue, just messy)"

if $DUPLICATE_LOCKFILE_WARN && ! $JSON_MODE; then
    while IFS= read -r -d '' pkg; do
        proj_dir="$(dirname "$pkg")"
        [[ "$proj_dir" == *"/node_modules/"* ]] && continue

        locks_found=()
        [[ -f "$proj_dir/package-lock.json" ]] && locks_found+=("npm")
        [[ -f "$proj_dir/yarn.lock" ]] && locks_found+=("yarn")
        [[ -f "$proj_dir/pnpm-lock.yaml" ]] && locks_found+=("pnpm")
        [[ -f "$proj_dir/bun.lockb" ]] && locks_found+=("bun")

        if (( ${#locks_found[@]} > 1 )); then
            printf "  ${YELLOW}WARNING${RESET}  %-50s  Multiple lockfiles: %s\n" \
                "${proj_dir#"$TARGET_DIR"/}" "${locks_found[*]}"
        fi
    done < <(find "$TARGET_DIR" -name "package.json" -not -path "*/node_modules/*" -print0 2>/dev/null)
fi

# ---------------------------------------------------------------------------
# 10. Core dumps and crash artifacts
# ---------------------------------------------------------------------------
section_header "CRASH ARTIFACTS" "Core dumps and other crash debris"

while IFS= read -r -d '' f; do
    size_bytes=$(file_size_bytes "$f")
    size_mb=$(( size_bytes / 1048576 ))
    if (( size_mb >= 1 )); then
        record_finding "CRASH" "$f" "$size_bytes" "Core dump / crash artifact"
        maybe_delete "$f" "core dump"
    fi
done < <(find "$TARGET_DIR" \( -name "core" -o -name "core.*" -o -name "hs_err_pid*" \
    -o -name "*.dmp" -o -name "*.mdmp" \) -type f -print0 2>/dev/null)

# ---------------------------------------------------------------------------
# Summary
# ---------------------------------------------------------------------------
if $JSON_MODE; then
    echo "{"
    echo "  \"target\": \"$TARGET_DIR\","
    echo "  \"total_waste_bytes\": $TOTAL_WASTE,"
    echo "  \"total_waste_human\": \"$(human_size $TOTAL_WASTE)\","
    echo "  \"findings\": ["
    for i in "${!FINDINGS[@]}"; do
        if (( i < ${#FINDINGS[@]} - 1 )); then
            echo "    ${FINDINGS[$i]},"
        else
            echo "    ${FINDINGS[$i]}"
        fi
    done
    echo "  ]"
    echo "}"
else
    echo ""
    echo "============================================================================"
    printf "${BOLD}Total reclaimable space found: ${RED}%s${RESET}\n" "$(human_size $TOTAL_WASTE)"
    echo "============================================================================"

    if (( TOTAL_WASTE == 0 )); then
        printf "${GREEN}Looks clean. No obvious waste found above configured thresholds.${RESET}\n"
    else
        echo ""
        printf "${DIM}Thresholds: node_modules=%dMB  venv=%dMB  build=%dMB  log=%dMB  cache=%dMB  binary=%dMB  media=%dMB${RESET}\n" \
            "$NODE_MODULES_THRESHOLD_MB" "$VENV_THRESHOLD_MB" "$BUILD_DIR_THRESHOLD_MB" \
            "$LOG_FILE_THRESHOLD_MB" "$CACHE_DIR_THRESHOLD_MB" "$LARGE_BINARY_THRESHOLD_MB" \
            "$LARGE_MEDIA_THRESHOLD_MB"
        printf "${DIM}Re-run with --delete to interactively remove items, or --json for machine-readable output.${RESET}\n"
    fi
fi
